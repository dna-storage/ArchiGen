#!/usr/bin/env python3.11

import sys
import random
import argparse
import matplotlib.pyplot as plt
import pandas as pd

from DNAStorage import *
from DNAStorageWithCache import *

parser = argparse.ArgumentParser(description='''
When reads follow hints, it is usually same file for read_trace and write_trace.
When they are not, it is usually *_hint.txt and *_outside_hint, respectively. 
Technically both files are generated by gen.py as trace with writes, then reads, but with different seeds.
Hence, we just ignore write part of outside_hint and use the read part only
''')

parser.add_argument('read_trace')
parser.add_argument('mode')
parser.add_argument('write_trace')
parser.add_argument('prefix')
args = parser.parse_args()

N_ATTEMPTS = 100

N_SAMPLES = 200
N_SAMPLES_HOT = None
# It was 10 previously, while N_SAMPLES=30. There is no reason to resynthesize less copies of the exhausted container.
RESYNTH = N_SAMPLES
# Here we resynthesize with one copy left. 
# However, it means that there is only one sample left overall. 
# So, we amplify ALL files from ONE sample, which implies we use multiplex PCR or flanking primers
RESYNTH_THRESHOLD = 1

YEARS = 30
SCRUBBING_READS_PEAR_YEAR = 0
READS_PER_YEAR = 500 + SCRUBBING_READS_PEAR_YEAR

WRITES_PER_YEAR = 10000
WRITE_MAX_YEAR = 1

random.seed(0x0123456789ABCDEF)

if args.mode == "fig3":
	configs = [
		{"blocks_per_pool": 1, "placement_policy": RANDOM, "cache": False},
		{"blocks_per_pool": 100, "placement_policy": RANDOM, "cache": False},
		{"blocks_per_pool": 1000, "placement_policy": RANDOM, "cache": False},
	]

elif args.mode == "fig4":
	configs = [
		{"blocks_per_pool": 1000, "placement_policy": RANDOM, "cache": False},
		{"blocks_per_pool": 1000, "placement_policy": DISTRIBUTE_HOT, "cache": False},
		{"blocks_per_pool": 1000, "placement_policy": HOT_TOGETHER, "cache": False},
		{"blocks_per_pool": 1000, "placement_policy": HOT_TOGETHER, "cache": False, "n_samples_hot": 1600}
	]

elif args.mode == "fig6":

	CACHE_SIZE_005 = int(10000 * 0.005) # 0.5% of total storage capacity
	assert(CACHE_SIZE_005 == 50)

	CACHE_SIZE_01 = int(10000 * 0.01) # 1.0% of total storage capacity
	assert(CACHE_SIZE_01 == 100)

	configs = [
		{"blocks_per_pool": 1000, "placement_policy": RANDOM, "cache": True, "cache_size": CACHE_SIZE_005},
		{"blocks_per_pool": 1000, "placement_policy": DISTRIBUTE_HOT, "cache": True, "cache_size": CACHE_SIZE_005},
		{"blocks_per_pool": 1000, "placement_policy": HOT_TOGETHER, "cache": True, "cache_size": CACHE_SIZE_005},
		{"blocks_per_pool": 1000, "placement_policy": RANDOM, "cache": HOT_ONLY, "cache_size": CACHE_SIZE_005},
		{"blocks_per_pool": 1000, "placement_policy": HOT_TOGETHER, "cache": True, "cache_size": CACHE_SIZE_005, "n_samples_hot": 1600},
		{"blocks_per_pool": 1000, "placement_policy": RANDOM, "cache": True, "cache_size": CACHE_SIZE_01},
	]

else:
	assert False

block_reads = []
rebuild_requests = []
for col, config in enumerate(configs):

	BLOCKS_PER_POOL = config["blocks_per_pool"]
	grouping = config["placement_policy"]
	
	cache = config["cache"]
	CACHE_SIZE = config.get("cache_size")

	N_SAMPLES_HOT = config.get("n_samples_hot")
	RESYNTH_HOT = config.get("resynth_hot", N_SAMPLES_HOT)
			
	attempt_block_reads = []
	attempt_rebuild_requests = []
	for attempt in range(N_ATTEMPTS):
		with open(args.read_trace) as f, open(args.write_trace) as wr_f:
			for y in range(YEARS):
				if y < WRITE_MAX_YEAR:
					# TODO: fix, unsafe
					zipf20 = eval(wr_f.readline())

					writes = [int(wr_f.readline().split()[1]) for _ in range(WRITES_PER_YEAR)]

					# TODO: think
					# seed = RANDOM_SEEDS.pop(0)
					seed = None
					if cache:
						st = DNAStorageWithCache(YEARS, grouping, BLOCKS_PER_POOL, N_SAMPLES, RESYNTH_THRESHOLD, N_SAMPLES, zipf20, writes, seed, N_SAMPLES_HOT, N_SAMPLES_HOT, cache, CACHE_SIZE)
					elif not cache:
						st = DNAStorage(YEARS, grouping, BLOCKS_PER_POOL, N_SAMPLES, RESYNTH_THRESHOLD, N_SAMPLES, zipf20, writes, seed, N_SAMPLES_HOT, N_SAMPLES_HOT)
					else:
						assert 0, "Fix HOT_ONLY cache"

					f.readline()
					# skip same number of lines in file with read accesses
					_ = [int(f.readline().split()[1]) for _ in range(WRITES_PER_YEAR)]


				for _ in range(READS_PER_YEAR):
					line = f.readline().split()
					assert line[0] == "r" or line[0] == "s", line[0]
					block = int(line[1])

					st.read(y, block)

			t = f.read()
			assert t == '', f"Trace is not finished"

		attempt_block_reads.append(sorted(st.block_reads, reverse=True))

		cum_rebuild_requests = [0] * YEARS
		cum_rebuild_requests[0] = st.rebuild_requests[0]
		for i in range(1, len(st.rebuild_requests)):
			cum_rebuild_requests[i] = cum_rebuild_requests[i - 1] + st.rebuild_requests[i]

		attempt_rebuild_requests.append(cum_rebuild_requests)

	block_reads.append((config, attempt_block_reads))

	rebuild_requests.append((config, attempt_rebuild_requests))

processed_block_reads = {}
for config, attempt_block_reads in block_reads:
	n_blocks = len(attempt_block_reads[0])

	min_block_reads = [1_000_000_000] * n_blocks
	max_block_reads = [0] * n_blocks
	mean_block_reads = [0] * n_blocks
	for attempt in attempt_block_reads:
		for y in range(n_blocks):
			min_block_reads[y] = min(min_block_reads[y], attempt[y])
			max_block_reads[y] = max(max_block_reads[y], attempt[y])
			mean_block_reads[y] += attempt[y]

	for i in range(len(mean_block_reads)):
		mean_block_reads[i] /= len(attempt_block_reads)

	# print("*** CONFIG:", config)
	# print("MIN")
	# print(min_block_reads)
	# print("MAX")
	# print(max_block_reads)
	# print("MEAN")
	# print(mean_block_reads)

	processed_block_reads[str(config)] = mean_block_reads

df = pd.DataFrame(zip(*processed_block_reads.values()), columns=processed_block_reads.keys())

df.to_csv(f"{args.prefix}_block_reads.csv")


processed_rebuild_requests = {}
for config, attempt_rebuild_requests in rebuild_requests:

	min_rebuild_requests = [1_000_000_000] * YEARS
	max_rebuild_requests = [0] * YEARS
	mean_rebuild_requests = [0] * YEARS
	for attempt in attempt_rebuild_requests:
		assert len(attempt) == YEARS, f"{len(attempt)} != {YEARS}" 
		for y in range(YEARS):
			min_rebuild_requests[y] = min(min_rebuild_requests[y], attempt[y])
			max_rebuild_requests[y] = max(max_rebuild_requests[y], attempt[y])
			mean_rebuild_requests[y] += attempt[y]

	for i in range(len(mean_rebuild_requests)):
		mean_rebuild_requests[i] /= len(attempt_rebuild_requests)

	# print("*** CONFIG:", config)
	# print("MIN")
	# print(min_rebuild_requests)
	# print("MAX")
	# print(max_rebuild_requests)
	# print("MEAN")
	# print(mean_rebuild_requests)

	# print("time to first rebuild", min(y for y in range(len(curve)) if curve[y] > 0))
	# print("total rebuilt blocks", curve[-1])

	processed_rebuild_requests[str(config)] = mean_rebuild_requests

df = pd.DataFrame(zip(*processed_rebuild_requests.values()), columns=processed_rebuild_requests.keys())

df.to_csv(f"{args.prefix}_rebuild_requests.csv")

plt.show()